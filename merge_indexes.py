#!/usr/bin/env python3
"""
Merge partial indexes from distributed workers into a single unified index.

This script combines the partial .parquet and .faiss files generated by
multiple workers into a single consolidated index.

Usage:
  python merge_indexes.py \
      --index-prefix /Volumes/home/joe/imageindex \
      --num-workers 4
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import List, Dict

import numpy as np
import polars as pl
import faiss


# --------- Utility functions --------- #


def log(msg: str) -> None:
    """Log message with timestamp."""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [Merge] {msg}", flush=True)


def load_metadata(meta_path: Path) -> dict:
    with meta_path.open("r", encoding="utf-8") as f:
        return json.load(f)


def save_metadata(meta_path: Path, data: dict) -> None:
    with meta_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)


# --------- Merging Logic --------- #


def cmd_merge(args: argparse.Namespace) -> None:
    start_time = time.time()

    prefix = Path(args.index_prefix).expanduser().resolve()
    num_workers = args.num_workers

    log(f"Merging indexes from {num_workers} workers")
    log(f"Index prefix: {prefix}")

    # Collect worker files
    worker_files = []
    for worker_id in range(num_workers):
        worker_prefix = prefix.parent / f"worker_{worker_id}"
        parquet_path = worker_prefix.with_suffix(".parquet")
        faiss_path = worker_prefix.with_suffix(".faiss")
        meta_path = worker_prefix.with_suffix(".meta.json")

        if not parquet_path.exists():
            log(f"ERROR: Worker {worker_id} parquet file not found: {parquet_path}")
            sys.exit(1)

        if not faiss_path.exists():
            log(f"ERROR: Worker {worker_id} faiss file not found: {faiss_path}")
            sys.exit(1)

        if not meta_path.exists():
            log(f"ERROR: Worker {worker_id} metadata file not found: {meta_path}")
            sys.exit(1)

        worker_files.append({
            "worker_id": worker_id,
            "parquet": parquet_path,
            "faiss": faiss_path,
            "meta": meta_path,
        })

    log(f"Found all {num_workers} worker index files")

    # Load and validate metadata
    log("Loading worker metadata...")
    worker_metas = []
    embedding_dim = None
    model_name = None
    total_images = 0
    total_failed = 0
    total_processing_time = 0

    for wf in worker_files:
        meta = load_metadata(wf["meta"])
        worker_metas.append(meta)

        # Validate consistency
        if embedding_dim is None:
            embedding_dim = meta["embedding_dim"]
            model_name = meta["model_name"]
        else:
            if meta["embedding_dim"] != embedding_dim:
                log(f"ERROR: Worker {wf['worker_id']} has mismatched embedding dim: {meta['embedding_dim']} vs {embedding_dim}")
                sys.exit(1)
            if meta["model_name"] != model_name:
                log(f"WARNING: Worker {wf['worker_id']} used different model: {meta['model_name']} vs {model_name}")

        total_images += meta["num_images"]
        total_failed += meta.get("num_failed", 0)
        total_processing_time += meta.get("processing_time_seconds", 0)

        log(f"  Worker {wf['worker_id']}: {meta['num_images']} images, dim={meta['embedding_dim']}")

    log(f"Total images across all workers: {total_images}")
    log(f"Total failed images: {total_failed}")
    log(f"Embedding dimension: {embedding_dim}")

    # Merge Parquet files
    log("\nMerging Parquet files...")
    dfs = []
    for wf in worker_files:
        log(f"  Loading {wf['parquet']}...")
        df = pl.read_parquet(wf["parquet"])
        dfs.append(df)

    log("Concatenating dataframes...")
    merged_df = pl.concat(dfs)

    if len(merged_df) != total_images:
        log(f"WARNING: Merged dataframe has {len(merged_df)} rows, expected {total_images}")

    # Save merged Parquet
    merged_parquet = prefix.with_suffix(".parquet")
    log(f"Writing merged Parquet to: {merged_parquet}")
    merged_df.write_parquet(merged_parquet)
    log(f"  Wrote {len(merged_df)} rows")

    # Merge Faiss indexes
    log("\nMerging Faiss indexes...")

    # Create new empty index
    log(f"Creating new Faiss index (dim={embedding_dim})...")
    merged_index = faiss.IndexFlatIP(embedding_dim)

    # Load and add each worker's vectors
    for wf in worker_files:
        log(f"  Loading and adding vectors from {wf['faiss']}...")
        worker_index = faiss.read_index(str(wf["faiss"]))

        if worker_index.d != embedding_dim:
            log(f"ERROR: Worker {wf['worker_id']} faiss index has wrong dimension: {worker_index.d} vs {embedding_dim}")
            sys.exit(1)

        # Extract vectors from worker index
        worker_vectors = faiss.vector_to_array(
            faiss.downcast_index(worker_index.index if hasattr(worker_index, 'index') else worker_index).get_xb()
        ).reshape(-1, embedding_dim)

        merged_index.add(worker_vectors)
        log(f"    Added {worker_vectors.shape[0]} vectors (total now: {merged_index.ntotal})")

    # Save merged Faiss index
    merged_faiss = prefix.with_suffix(".faiss")
    log(f"Writing merged Faiss index to: {merged_faiss}")
    faiss.write_index(merged_index, str(merged_faiss))
    log(f"  Index contains {merged_index.ntotal} vectors")

    # Create merged metadata
    log("\nCreating merged metadata...")
    merged_meta = {
        "model_name": model_name,
        "embedding_dim": embedding_dim,
        "num_images": int(merged_index.ntotal),
        "num_failed": total_failed,
        "num_workers": num_workers,
        "total_processing_time_seconds": total_processing_time,
        "merge_time_seconds": time.time() - start_time,
        "worker_metadata": worker_metas,
    }

    merged_meta_path = prefix.with_suffix(".meta.json")
    log(f"Writing merged metadata to: {merged_meta_path}")
    save_metadata(merged_meta_path, merged_meta)

    # Verify integrity
    log("\nVerifying merged index integrity...")

    # Check that parquet and faiss have same number of entries
    if len(merged_df) != merged_index.ntotal:
        log(f"ERROR: Mismatch between Parquet ({len(merged_df)}) and Faiss ({merged_index.ntotal}) sizes!")
        sys.exit(1)

    log("  Integrity check passed!")

    # Summary
    total_time = time.time() - start_time
    log("\n" + "="*60)
    log("MERGE COMPLETE!")
    log(f"  Total images: {merged_index.ntotal}")
    log(f"  Failed images: {total_failed}")
    log(f"  Merge time: {total_time:.1f} seconds")
    log(f"  Files created:")
    log(f"    - {merged_parquet}")
    log(f"    - {merged_faiss}")
    log(f"    - {merged_meta_path}")
    log("="*60)


# --------- Main CLI --------- #


def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="Merge partial indexes from distributed workers."
    )

    p.add_argument(
        "--index-prefix",
        required=True,
        help="Prefix for final merged index (e.g., /path/to/imageindex).",
    )
    p.add_argument(
        "--num-workers",
        type=int,
        required=True,
        help="Number of workers that generated partial indexes.",
    )

    return p


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()
    cmd_merge(args)


if __name__ == "__main__":
    main()
